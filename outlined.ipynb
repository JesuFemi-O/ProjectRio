{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "outlined.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddV5VQ7vllrl"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFjcrjRpuDaV"
      },
      "source": [
        "\r\n",
        "json_file = {\"intents\": [\r\n",
        "    {\"tag\": \"greetings\",\r\n",
        "    \"patterns\": [\"hello\", \"hey\", \"hi\", \"How far?\", \"My guy!\"],\r\n",
        "    \"responses\": [\"Hello!\", \"hey!\", \"hey there, what can i do for you?\"]\r\n",
        "    },\r\n",
        "\r\n",
        "    {\"tag\": \"goodbye\",\r\n",
        "     \"patterns\": [\"cya\", \"see you later\", \"goodbye\", \"got to go\", \"see ya!\"],\r\n",
        "     \"responses\": [\"nice chatting with you\", \"talk to you soon, cheers\"]\r\n",
        "    },\r\n",
        "\r\n",
        "    {\"tag\": \"age\",\r\n",
        "     \"patterns\": [\"how old are you\", \"what's your age?\", \"Age\"],\r\n",
        "     \"responses\": [\"just a few days old, still have a lot to learn\", \"less than a week old\"]\r\n",
        "    },\r\n",
        "    \r\n",
        "    {\"tag\": \"name\",\r\n",
        "     \"patterns\": [\"what is your name\", \"what should i call you\", \"what's your name\", \"can you tell me your name?\"],\r\n",
        "     \"responses\": [\"you can call me lisa\", \"I'm lisa!\", \"Elizabeth. but please call me lisa *winks*\"]\r\n",
        "    },\r\n",
        "\r\n",
        "    {\"tag\": \"shop\",\r\n",
        "     \"patterns\": [\"I' like to buy something\", \"what are your products\", \"what do you recommend?\", \"what are you selling\"],\r\n",
        "     \"responses\": [\"we sell samosa, spring rolls, chocoloate and vanilla cakes, and also bake anything you want on demand!\"]\r\n",
        "    },\r\n",
        "\r\n",
        "    {\"tag\": \"hours\",\r\n",
        "     \"patterns\": [\"when are you guys open\", \"what are your hours\", \"hours of opening?\"],\r\n",
        "     \"responses\": [\"we are open at all times\", \"24/7\"]\r\n",
        "    }\r\n",
        "   ]\r\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ldAPB6OuFdR"
      },
      "source": [
        "def flatten_json(json_file):\r\n",
        "  tags = []\r\n",
        "  pattern_corpus = []\r\n",
        "  responses = []\r\n",
        "  for intent in json_file['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "        pattern_corpus.append(pattern)\r\n",
        "        tags.append(intent['tag'])\r\n",
        "        responses.append(intent['responses'])\r\n",
        "  if len(pattern_corpus) != len(tags):\r\n",
        "    print('patterns do not match tags')\r\n",
        "    raise\r\n",
        "  data = {\"tags\":tags,\"pattern\":pattern_corpus,\"responses\":responses}\r\n",
        "  return tags, pattern_corpus, responses"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWp-FSFjfaxw"
      },
      "source": [
        "def clean_corpus(sents):\r\n",
        "    stemmer = nltk.stem.LancasterStemmer()\r\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\r\n",
        "    corpus = []\r\n",
        "    for doc in sents:\r\n",
        "        tokens = nltk.word_tokenize(doc)\r\n",
        "        filtered_tokens = [lemmatizer.lemmatize(token) if token not in ignore_words else token for token in tokens ]\r\n",
        "        doc = ' '.join(filtered_tokens)\r\n",
        "        corpus.append(doc)\r\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnUIlevXfqVz"
      },
      "source": [
        "def create_bow(sents):\r\n",
        "    cv = CountVectorizer(min_df=0., max_df=1.)\r\n",
        "    bow = cv.fit_transform(corpus)\r\n",
        "    bow = bow.toarray()\r\n",
        "    return bow, cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2KQwZzMfnbJ"
      },
      "source": [
        "def ohe_labels(target_list):\r\n",
        "    le = LabelEncoder()\r\n",
        "    intent_label = le.fit_transform(target_list)\r\n",
        "\r\n",
        "    encoder = OneHotEncoder(sparse=False) #take good note of this!! sparse set to False is the secret here\r\n",
        "\r\n",
        "    intent_label = intent_label.reshape((-1, 1))\r\n",
        "\r\n",
        "    intent_label = encoder.fit_transform(intent_label)\r\n",
        "    return intent_label, le"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}