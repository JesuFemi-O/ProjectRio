{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prepare data for an intent classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some basic considerations to be taken for a chatbot model.\n",
    "\n",
    "\n",
    "Firstly, the model needs inputs/patterns which is the message the user would send to the chatbot\n",
    "\n",
    "the model also requires corresponding targets for this inputs...\n",
    "\n",
    "\n",
    "We would use a json file in the long run which is the norm in industry, but there's a basic problem to understand\n",
    "\n",
    "1. the user messages has to be used to form a corpus\n",
    "\n",
    "2. this so called corpus is actually just a list of sentence where every index position is a new row/document\n",
    "\n",
    "3. we have to convert this corpus into a bag of words\n",
    "\n",
    "4. the target is also a list where each label/intent is for a corresponding document at the same index position in the list of words.\n",
    "\n",
    "5. we will have to one hot encode this list\n",
    "\n",
    "\n",
    "this notebook explores efficient and easy to understand approach to getting these tasks done..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['hello',\n",
    "            'hi',\n",
    "            'what do you sell?',\n",
    "            'what services do you render',\n",
    "            'I would love 3 spring rolls',\n",
    "            'can i get a bottle of coke?',\n",
    "            'thank you',\n",
    "            'goodbye',\n",
    "            'cheers!']\n",
    "intents = ['greetings',\n",
    "          'greetings',\n",
    "          'services',\n",
    "          'services',\n",
    "          'order',\n",
    "          'order',\n",
    "          'farewell',\n",
    "          'farewell',\n",
    "          'farewell',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sentences) == len(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming sentences to bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenize\n",
    "\n",
    "- lemmatize\n",
    "\n",
    "- create vocabulary\n",
    "\n",
    "- create bow\n",
    "\n",
    "- convert bow list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "ignore_words = ['?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(sents):\n",
    "    corpus = []\n",
    "    for doc in sents:\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) if token not in ignore_words else token for token in tokens ]\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        corpus.append(doc)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hi',\n",
       " 'what do you sell ?',\n",
       " 'what service do you render',\n",
       " 'I would love 3 spring roll',\n",
       " 'can i get a bottle of coke ?',\n",
       " 'thank you',\n",
       " 'goodbye',\n",
       " 'cheer !']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = clean_corpus(sentences)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0., max_df=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = cv.fit_transform(corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for pattern in sentences:\n",
    "    w = nltk.word_tokenize(pattern)\n",
    "    # add to our words list\n",
    "    words.extend(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bottle',\n",
       " 'can',\n",
       " 'cheer',\n",
       " 'coke',\n",
       " 'do',\n",
       " 'get',\n",
       " 'goodbye',\n",
       " 'hello',\n",
       " 'hi',\n",
       " 'love',\n",
       " 'of',\n",
       " 'render',\n",
       " 'roll',\n",
       " 'sell',\n",
       " 'service',\n",
       " 'spring',\n",
       " 'thank',\n",
       " 'what',\n",
       " 'would',\n",
       " 'you']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"this is me being cool for the coke. hello?\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have successfully reduced complexity around creating bag of word... next thing is one hot encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "intent_labels = le.fit_transform(intents)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False) #take good note of this!! sparse set to False is the secret here\n",
    "\n",
    "intent_labels = intent_labels.reshape((-1, 1))\n",
    "\n",
    "intent_labels = encoder.fit_transform(intent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3], dtype=int64)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'services'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been able to successfully one hot encode without writing a long and confusing code... I jsut need to pickle my vectoorizer, label encoder and one_hot_encoder for transformation purposes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating corpus from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = {\"intents\": [\n",
    "    {\"tag\": \"greetings\",\n",
    "    \"patterns\": [\"hello\", \"hey\", \"hi\", \"How far?\", \"My guy!\"],\n",
    "    \"responses\": [\"Hello!\", \"hey!\", \"hey there, what can i do for you?\"]\n",
    "    },\n",
    "\n",
    "    {\"tag\": \"goodbye\",\n",
    "     \"patterns\": [\"cya\", \"see you later\", \"goodbye\", \"got to go\", \"see ya!\"],\n",
    "     \"responses\": [\"nice chatting with you\", \"talk to you soon, cheers\"]\n",
    "    },\n",
    "\n",
    "    {\"tag\": \"age\",\n",
    "     \"patterns\": [\"how old are you\", \"what's your age?\", \"Age\"],\n",
    "     \"responses\": [\"just a few days old, still have a lot to learn\", \"less than a week old\"]\n",
    "    },\n",
    "    \n",
    "    {\"tag\": \"name\",\n",
    "     \"patterns\": [\"what is your name\", \"what should i call you\", \"what's your name\", \"can you tell me your name?\"],\n",
    "     \"responses\": [\"you can call me lisa\", \"I'm lisa!\", \"Elizabeth. but please call me lisa *winks*\"]\n",
    "    },\n",
    "\n",
    "    {\"tag\": \"shop\",\n",
    "     \"patterns\": [\"I' like to buy something\", \"what are your products\", \"what do you recommend?\", \"what are you selling\"],\n",
    "     \"responses\": [\"we sell samosa, spring rolls, chocoloate and vanilla cakes, and also bake anything you want on demand!\"]\n",
    "    },\n",
    "\n",
    "    {\"tag\": \"hours\",\n",
    "     \"patterns\": [\"when are you guys open\", \"what are your hours\", \"hours of opening?\"],\n",
    "     \"responses\": [\"we are open at all times\", \"24/7\"]\n",
    "    }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "pattern_corpus = []\n",
    "\n",
    "for intent in json_file['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        pattern_corpus.append(pattern)\n",
    "        tags.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pattern_corpus) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'How far?',\n",
       " 'My guy!',\n",
       " 'cya',\n",
       " 'see you later',\n",
       " 'goodbye',\n",
       " 'got to go',\n",
       " 'see ya!',\n",
       " 'how old are you',\n",
       " \"what's your age?\",\n",
       " 'Age',\n",
       " 'what is your name',\n",
       " 'what should i call you',\n",
       " \"what's your name\",\n",
       " 'can you tell me your name?',\n",
       " \"I' like to buy something\",\n",
       " 'what are your products',\n",
       " 'what do you recommend?',\n",
       " 'what are you selling',\n",
       " 'when are you guys open',\n",
       " 'what are your hours',\n",
       " 'hours of opening?']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow(sents):\n",
    "    cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "    bow = cv.fit_transform(corpus)\n",
    "    bow = bow.toarray()\n",
    "    return bow, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_labels(target_list):\n",
    "    le = LabelEncoder()\n",
    "    intent_label = le.fit_transform(target_list)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False) #take good note of this!! sparse set to False is the secret here\n",
    "\n",
    "    intent_label = intent_label.reshape((-1, 1))\n",
    "\n",
    "    intent_label = encoder.fit_transform(intent_label)\n",
    "    return intent_label, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clean_corpus(pattern_corpus)\n",
    "matrix, vec = create_bow(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, le_vec = ohe_labels(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working in production environment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before i can pass anything to the model i woukd first of all need it pass thru all my functions for cleaniing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = clean_corpus(['hello there my really cool and amazingly running guy'])\n",
    "msg = vec.transform(msg).toarray()\n",
    "\n",
    "#i can now pass the above to my model\n",
    "\n",
    "#after the model makes a prediction i jsut need to\n",
    "\n",
    "# - use argmax to find the index postion e.g idx = np.argmax(model.predict(msg)[0])\n",
    "\n",
    "# i can then use le_vec.classes_[idx] -> this gives me the  intent class so i can use the JSON to pick a response...great Job emmanuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using this argmax idea eliminates the log process of trying to sort the list... instead i can get the argmax index and value and if the value is less than my error threshold the model will simply say it does not understand the user input/request else it just uses the index value to pick a randomo response attending to the user immediately..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
